Notes for working on the ufs2arco:
	task 1: add the GDAS reanalysis file to the package 

* installing the package with contribution mode on Ursa
	1. create and activate a development environment
		module purge
		cd ufs2arco
		conda env create -f environment.yaml
		conda activate ufs2arco
		pip install -e . --no-deps
	(some of commend: conda env list; conda remove --name ufs2arco --all; conda clean --all --yes; pip cache purge)
	   -- install mpi4py: conda install -c conda-forge mpi4py
	   
    2. pytest to test the package (32 passed, 47 warnings in 419.41s (0:06:59))

* modify the gfs related codes and documents to gdas
	1. add the source code to process the gdas file
       -- ufs2arco/sources/noaa_grib_forecast.py: add ‘gdas’ attribution (done)
       -- ufs2arco/sources/__init__.py: add 'gdas' attribution (done)
	   -- ufs2arco/sources/gdas_archive.py: cp from gfs_archive.py and make the changes (done)
	   -- ufs2arco/sources/reference.gdas.yaml: cp from reference.gfs.yaml and make the changes (done)
	   
	2. tesing the added code with pytest
	   -- write the ufs2arco/tests/test_gdasdataset.py  (done)
	   -- test run: pytest test_gdasdataset.py   (pass)
	   
	3. document the gdas module
	   -- adding 'gdas' to create_variable_tables.py, and run the code to generate the variables table of gdas (done)
	   -- cp gfs_archive.rst and modify to the gdas_archive.rst (done)
		
    Bug: (?) the processing has the reading error of the ‘ .idx’ file; sometimes it happens but sometimes does not.
	   -- fixed by modifing the 'slurm' script: node=1, ntask=1, cpu_per_task=4 and without ‘export OMP_NUM_THREADS=1’
 
* train the model with gdas data using the anemoi-training 
